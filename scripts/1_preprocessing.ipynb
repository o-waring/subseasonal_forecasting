{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIDSV-CePqcq"
   },
   "source": [
    "### Preprocessing Datasets\n",
    "\n",
    "Here we load, process, and save back to drive all h5 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VcjucHYIsiu"
   },
   "source": [
    "**Mount Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9192,
     "status": "ok",
     "timestamp": 1594285211049,
     "user": {
      "displayName": "Oliver Waring",
      "photoUrl": "",
      "userId": "08288196707116761281"
     },
     "user_tz": -60
    },
    "id": "4Ixea-QMesrd",
    "outputId": "ba0371ed-5f92-4d73-c098-74920ec624cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tables\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/c3/8fd9e3bb21872f9d69eb93b3014c86479864cca94e625fd03713ccacec80/tables-3.6.1-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3MB 8.6MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from tables) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: numexpr>=2.6.2 in /usr/local/lib/python3.6/dist-packages (from tables) (2.7.1)\n",
      "Installing collected packages: tables\n",
      "  Found existing installation: tables 3.4.4\n",
      "    Uninstalling tables-3.4.4:\n",
      "      Successfully uninstalled tables-3.4.4\n",
      "Successfully installed tables-3.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27372,
     "status": "ok",
     "timestamp": 1595236723429,
     "user": {
      "displayName": "Oliver Waring",
      "photoUrl": "",
      "userId": "08288196707116761281"
     },
     "user_tz": -60
    },
    "id": "HOfNsuHcIeBf",
    "outputId": "5474bcb6-0f12-412e-cf98-03ce304513ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "repo_path = \"/content/drive/My Drive/repos/subseasonal_rodeo/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wx4mJY2_FouM"
   },
   "source": [
    "**Pre-Processing of Core Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pbjSHo7SFouN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# %matplotlib inline\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xb74b4Vm5NLb"
   },
   "source": [
    "**Load & Process Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nR3LGjeT2ce"
   },
   "outputs": [],
   "source": [
    "def process_series(data_path):\n",
    "  data = pd.read_hdf(repo_path+data_path)\n",
    "  data = pd.DataFrame(data.reset_index(drop=False))\n",
    "  data = data[data['start_date'] > datetime.datetime.strptime('1978-12-31',\"%Y-%m-%d\")].reset_index(drop=True)\n",
    "  return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySvMrTMaMdK3"
   },
   "outputs": [],
   "source": [
    "# Series datasets\n",
    "pres = process_series('data/gt-contest_pres.sfc.gauss-14d-1948-2018.h5') # Pressure\n",
    "hgt = process_series('data/gt-contest_wind_hgt_10-14d-1948-2018.h5') # Wind @10m geopotential height\n",
    "rhum = process_series('data/gt-contest_rhum.sig995-14d-1948-2018.h5') # Relative humidity\n",
    "slp = process_series('data/gt-contest_slp-14d-1948-2018.h5') # Sea level pressure\n",
    "prec = process_series('data/gt-contest_precip-14d-1948-2018.h5') # Precipitation\n",
    "prwtr = process_series('data/gt-contest_pr_wtr.eatm-14d-1948-2018.h5') # Precipitable water\n",
    "pevpr = process_series('data/gt-contest_pevpr.sfc.gauss-14d-1948-2018.h5') # Potential evaporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-FsquQ1b8-n"
   },
   "outputs": [],
   "source": [
    "# Tmp2m dataset\n",
    "temp = pd.read_hdf(repo_path+'data/gt-contest_tmp2m-14d-1979-2018.h5')\n",
    "temp['start_date'] = pd.to_datetime(temp['start_date'], unit='ns')\n",
    "temp.drop(['tmp2m_sqd',\t'tmp2m_std'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UakvPTJUwec2"
   },
   "outputs": [],
   "source": [
    "# Tmp2m & Precipitation Climatology & Anomaly\n",
    "df = temp.merge(prec, left_on = ['lon','lat','start_date'], right_on = ['lon','lat','start_date'], how = 'inner')\n",
    "df['year'] = df['start_date'].dt.year\n",
    "df['month'] = df['start_date'].dt.month\n",
    "df['day'] = df['start_date'].dt.day\n",
    "df['dayofyear'] = df['start_date'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e56wBthM5elw"
   },
   "source": [
    "**Join all Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-JdZBvhyR7Zr"
   },
   "outputs": [],
   "source": [
    "# Merge all into single df\n",
    "# Spatial temporal datasets\n",
    "df = df.merge(pres, left_on = ['lon','lat','start_date'], right_on = ['lon','lat','start_date'], how = 'inner')\n",
    "del pres\n",
    "df = df.merge(hgt, left_on = ['lon','lat','start_date'], right_on = ['lon','lat','start_date'], how = 'inner')\n",
    "del hgt\n",
    "df = df.merge(rhum, left_on = ['lon','lat','start_date'], right_on = ['lon','lat','start_date'], how = 'inner')\n",
    "del rhum\n",
    "df = df.merge(slp, left_on = ['lon','lat','start_date'], right_on = ['lon','lat','start_date'], how = 'inner')\n",
    "del slp\n",
    "df = df.merge(prwtr, left_on = ['lon','lat','start_date'], right_on = ['lon','lat','start_date'], how = 'inner')\n",
    "del prwtr\n",
    "df = df.merge(pevpr, left_on = ['lon','lat','start_date'], right_on = ['lon','lat','start_date'], how = 'inner')\n",
    "del pevpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1594204883730,
     "user": {
      "displayName": "Oliver Waring",
      "photoUrl": "",
      "userId": "08288196707116761281"
     },
     "user_tz": -60
    },
    "id": "9GmMLd43MdOb",
    "outputId": "a2beb479-0685-4718-f084-651825e6d11f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>start_date</th>\n",
       "      <th>tmp2m</th>\n",
       "      <th>precip</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>pres</th>\n",
       "      <th>contest_wind_hgt_10</th>\n",
       "      <th>rhum</th>\n",
       "      <th>slp</th>\n",
       "      <th>pr_wtr</th>\n",
       "      <th>pevpr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>7.683595</td>\n",
       "      <td>8.122134</td>\n",
       "      <td>1979</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99634.573521</td>\n",
       "      <td>31083.037249</td>\n",
       "      <td>75.348356</td>\n",
       "      <td>102332.270229</td>\n",
       "      <td>16.637908</td>\n",
       "      <td>140.257407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>1979-01-02</td>\n",
       "      <td>7.283579</td>\n",
       "      <td>7.038652</td>\n",
       "      <td>1979</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99673.597796</td>\n",
       "      <td>31086.626395</td>\n",
       "      <td>72.662776</td>\n",
       "      <td>102404.656669</td>\n",
       "      <td>15.724481</td>\n",
       "      <td>144.829959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>1979-01-03</td>\n",
       "      <td>7.792389</td>\n",
       "      <td>7.038652</td>\n",
       "      <td>1979</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>99545.359933</td>\n",
       "      <td>31088.482282</td>\n",
       "      <td>74.123019</td>\n",
       "      <td>102301.680943</td>\n",
       "      <td>16.681810</td>\n",
       "      <td>139.518139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>1979-01-04</td>\n",
       "      <td>8.906203</td>\n",
       "      <td>7.118693</td>\n",
       "      <td>1979</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>99447.934710</td>\n",
       "      <td>31089.829381</td>\n",
       "      <td>77.210133</td>\n",
       "      <td>102189.965541</td>\n",
       "      <td>17.933701</td>\n",
       "      <td>141.163577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>1979-01-05</td>\n",
       "      <td>9.719387</td>\n",
       "      <td>7.325908</td>\n",
       "      <td>1979</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>99365.207310</td>\n",
       "      <td>31084.936942</td>\n",
       "      <td>77.090285</td>\n",
       "      <td>102108.830776</td>\n",
       "      <td>18.548535</td>\n",
       "      <td>147.473729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lat    lon start_date  ...            slp     pr_wtr       pevpr\n",
       "0  27.0  261.0 1979-01-01  ...  102332.270229  16.637908  140.257407\n",
       "1  27.0  261.0 1979-01-02  ...  102404.656669  15.724481  144.829959\n",
       "2  27.0  261.0 1979-01-03  ...  102301.680943  16.681810  139.518139\n",
       "3  27.0  261.0 1979-01-04  ...  102189.965541  17.933701  141.163577\n",
       "4  27.0  261.0 1979-01-05  ...  102108.830776  18.548535  147.473729\n",
       "\n",
       "[5 rows x 15 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UC_uXn7H5nci"
   },
   "source": [
    "**Feature Engineer Date Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PnA5xxHavyj"
   },
   "outputs": [],
   "source": [
    "df['month_sin'] = np.sin(df['month']*(2.*np.pi/12))\n",
    "df['month_cos'] = np.cos(df['month']*(2.*np.pi/12))\n",
    "df['dayofyear_sin'] = np.sin(df['dayofyear']*(2.*np.pi/366))\n",
    "df['dayofyear_cos'] = np.cos(df['dayofyear']*(2.*np.pi/366))\n",
    "# df.drop('start_month', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7684,
     "status": "ok",
     "timestamp": 1594204917519,
     "user": {
      "displayName": "Oliver Waring",
      "photoUrl": "",
      "userId": "08288196707116761281"
     },
     "user_tz": -60
    },
    "id": "24Po6_D6ey1y",
    "outputId": "e9443b44-71be-4fa8-f877-759ffee92f2c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>tmp2m</th>\n",
       "      <th>precip</th>\n",
       "      <th>pres</th>\n",
       "      <th>contest_wind_hgt_10</th>\n",
       "      <th>rhum</th>\n",
       "      <th>slp</th>\n",
       "      <th>pr_wtr</th>\n",
       "      <th>pevpr</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>dayofyear_sin</th>\n",
       "      <th>dayofyear_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>7.683595</td>\n",
       "      <td>8.122134</td>\n",
       "      <td>99634.570312</td>\n",
       "      <td>31083.037109</td>\n",
       "      <td>75.348358</td>\n",
       "      <td>102332.273438</td>\n",
       "      <td>16.637909</td>\n",
       "      <td>140.257401</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.999853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>27.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>8.215410</td>\n",
       "      <td>20.169296</td>\n",
       "      <td>100262.968750</td>\n",
       "      <td>31085.554688</td>\n",
       "      <td>78.908356</td>\n",
       "      <td>102335.476562</td>\n",
       "      <td>17.451164</td>\n",
       "      <td>132.152435</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.999853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>28.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>6.222338</td>\n",
       "      <td>12.087295</td>\n",
       "      <td>99910.593750</td>\n",
       "      <td>31073.150391</td>\n",
       "      <td>71.534996</td>\n",
       "      <td>102473.265625</td>\n",
       "      <td>14.227145</td>\n",
       "      <td>136.018433</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.999853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>28.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>6.348150</td>\n",
       "      <td>51.016727</td>\n",
       "      <td>100531.703125</td>\n",
       "      <td>31077.091797</td>\n",
       "      <td>75.604515</td>\n",
       "      <td>102458.054688</td>\n",
       "      <td>15.213875</td>\n",
       "      <td>126.014656</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.999853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>28.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>6.778420</td>\n",
       "      <td>116.063667</td>\n",
       "      <td>101668.062500</td>\n",
       "      <td>31084.351562</td>\n",
       "      <td>83.178177</td>\n",
       "      <td>102417.312500</td>\n",
       "      <td>17.226011</td>\n",
       "      <td>109.695877</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.999853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  start_date   lat    lon  ...  month_cos  dayofyear_sin  dayofyear_cos\n",
       "0 1979-01-01  27.0  261.0  ...   0.866025       0.017166       0.999853\n",
       "1 1979-01-01  27.0  262.0  ...   0.866025       0.017166       0.999853\n",
       "2 1979-01-01  28.0  261.0  ...   0.866025       0.017166       0.999853\n",
       "3 1979-01-01  28.0  262.0  ...   0.866025       0.017166       0.999853\n",
       "4 1979-01-01  28.0  263.0  ...   0.866025       0.017166       0.999853\n",
       "\n",
       "[5 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by date as primary sorting key so we can split for train/validation/test easily\n",
    "cols = list(df)\n",
    "cols.insert(0, cols.pop(cols.index('start_date')))\n",
    "df = df.loc[:, cols]\n",
    "df = df.sort_values(by=['start_date', 'lat','lon']).reset_index(drop=True)\n",
    "df = df.drop(['year','month','day','dayofyear'], axis=1)\n",
    "for col in df.columns[1:]:\n",
    "  df[col] = df[col].astype('float32')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4924,
     "status": "ok",
     "timestamp": 1594205057010,
     "user": {
      "displayName": "Oliver Waring",
      "photoUrl": "",
      "userId": "08288196707116761281"
     },
     "user_tz": -60
    },
    "id": "XrlLvCmM4UzY",
    "outputId": "7324820e-41bd-4d12-9065-43b7289d6e49"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>tmp2m</th>\n",
       "      <th>precip</th>\n",
       "      <th>pres</th>\n",
       "      <th>contest_wind_hgt_10</th>\n",
       "      <th>rhum</th>\n",
       "      <th>slp</th>\n",
       "      <th>pr_wtr</th>\n",
       "      <th>pevpr</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>dayofyear_sin</th>\n",
       "      <th>dayofyear_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>7.683595</td>\n",
       "      <td>8.122134</td>\n",
       "      <td>99634.570312</td>\n",
       "      <td>31083.037109</td>\n",
       "      <td>75.348358</td>\n",
       "      <td>102332.273438</td>\n",
       "      <td>16.637909</td>\n",
       "      <td>140.257401</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.999853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1979-01-02</td>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>7.283579</td>\n",
       "      <td>7.038651</td>\n",
       "      <td>99673.601562</td>\n",
       "      <td>31086.626953</td>\n",
       "      <td>72.662773</td>\n",
       "      <td>102404.656250</td>\n",
       "      <td>15.724482</td>\n",
       "      <td>144.829956</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.034328</td>\n",
       "      <td>0.999411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1979-01-03</td>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>7.792389</td>\n",
       "      <td>7.038651</td>\n",
       "      <td>99545.359375</td>\n",
       "      <td>31088.482422</td>\n",
       "      <td>74.123016</td>\n",
       "      <td>102301.679688</td>\n",
       "      <td>16.681810</td>\n",
       "      <td>139.518143</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.051479</td>\n",
       "      <td>0.998674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1979-01-04</td>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>8.906203</td>\n",
       "      <td>7.118693</td>\n",
       "      <td>99447.937500</td>\n",
       "      <td>31089.830078</td>\n",
       "      <td>77.210136</td>\n",
       "      <td>102189.968750</td>\n",
       "      <td>17.933701</td>\n",
       "      <td>141.163574</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.068615</td>\n",
       "      <td>0.997643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1979-01-05</td>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>9.719387</td>\n",
       "      <td>7.325908</td>\n",
       "      <td>99365.210938</td>\n",
       "      <td>31084.937500</td>\n",
       "      <td>77.090286</td>\n",
       "      <td>102108.828125</td>\n",
       "      <td>18.548536</td>\n",
       "      <td>147.473724</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.085731</td>\n",
       "      <td>0.996318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  start_date   lat    lon  ...  month_cos  dayofyear_sin  dayofyear_cos\n",
       "0 1979-01-01  27.0  261.0  ...   0.866025       0.017166       0.999853\n",
       "1 1979-01-02  27.0  261.0  ...   0.866025       0.034328       0.999411\n",
       "2 1979-01-03  27.0  261.0  ...   0.866025       0.051479       0.998674\n",
       "3 1979-01-04  27.0  261.0  ...   0.866025       0.068615       0.997643\n",
       "4 1979-01-05  27.0  261.0  ...   0.866025       0.085731       0.996318\n",
       "\n",
       "[5 rows x 15 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by date as primary sorting key so we can split for train/validation/test easily\n",
    "df = df.sort_values(by=['lat','lon', 'start_date']).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtlIORE3IPFA"
   },
   "source": [
    "**Save Unscaled Merged Dataset & Column Names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPP9w-1Jav5G"
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(df.columns.values).to_csv(repo_path+'data/processed/spatial_temporal/column_names.csv')\n",
    "# np.save(repo_path+'data/processed/spatial_temporal/full_data_unscaled_1979-2018', np.array(df))\n",
    "df = np.array(np.load(repo_path+'data/processed/spatial_temporal/full_data_unscaled_1979-2018.npy',allow_pickle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UncEpOQBwgK"
   },
   "source": [
    "**Conduct Preprocessing -- standardize, scale, transform to spatial 2D, mask missing regions, outer pad spatial tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIA_UxLIBwBN"
   },
   "outputs": [],
   "source": [
    "col_names = pd.read_csv(repo_path+'data/processed/spatial_temporal/column_names.csv')\n",
    "col_names = list(col_names['0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMVU1nRqCc5z"
   },
   "outputs": [],
   "source": [
    "locations = pd.read_csv(repo_path+'data/processed/spatial_temporal/target_points.csv')\n",
    "locations['region_id'] = list(zip(locations['lat'], locations['lon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sY05IoklCcyv"
   },
   "outputs": [],
   "source": [
    "class PreprocessTemporalSpatialData:\n",
    "    \"\"\"\n",
    "    Class for conducting preprocessing pipeline for temporal spatial data\n",
    "    Standardizes feature fields, and scales all categorical feature fiels\n",
    "    Transforms dataset to spatial form [timesteps,lat,lon,features]\n",
    "    Creates missing regions to enable above matrix transformation (filling in missing value with 0 - note this aligns well with [0,1] scaling)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data:np.array, locations:np.array, col_names:list, num_regions:int, num_features:int, max_sg:int=5):\n",
    "        self.data = data\n",
    "        self.locations = locations\n",
    "        self.regions = self.locations['region_id'].unique()\n",
    "        self.latmin, self.latmax, self.lonmin, self.lonmax = self.locations['lat'].min(), self.locations['lat'].max(), self.locations['lon'].min(), self.locations['lon'].max()\n",
    "        self.bin_width, self.bin_height = self.lonmax - self.lonmin + 1, self.latmax - self.latmin + 1\n",
    "        self.col_names = col_names\n",
    "        self.num_regions = num_regions\n",
    "        self.num_features = num_features\n",
    "        self.weather_features = 8\n",
    "        self.cyclical_features = 4\n",
    "        self.num_timesteps = self.data.reshape(self.num_regions, -1, self.num_features).shape[1]\n",
    "        self.max_sg = max_sg\n",
    "        \n",
    "    def standardize_and_scale_data(self, save=False):\n",
    "        \"\"\" Standarize features using mean, std from train set; then scale to 0-1 scale \"\"\"\n",
    "        # Ensure dataset is order by start date, by region (lat, then lon)\n",
    "#         self.data = self.data.sort_values(by=['start_date', 'lat','lon']).reset_index(drop=True)\n",
    "        # Reshape all regions together\n",
    "        self.data  = np.array(self.data).reshape(-1, self.num_features)\n",
    "        # Extract train_split - note we only standardize using mean and std from train set\n",
    "        TRAIN_SPLIT = self.num_timesteps #- 2*1008*self.num_regions\n",
    "        \n",
    "        # Start Date - need this for indexing/grouping by region\n",
    "        date = self.data[:,0].reshape((-1,1))\n",
    "\n",
    "        # Keep lat/lon from scaling\n",
    "        lat_lon = self.data[:,1:3].astype(np.float16)\n",
    "\n",
    "        # Standardize feature fields\n",
    "        features = self.data[:,3:-4].astype(np.float32)\n",
    "        self.data_mean = features[:TRAIN_SPLIT].mean(axis=0)\n",
    "        self.data_std = features[:TRAIN_SPLIT].std(axis=0)\n",
    "        features = ((features-self.data_mean)/self.data_std).astype(np.float16)\n",
    "        \n",
    "        # Deal with cyclical features separately - these are already scaled\n",
    "        cyclical_features = self.data[:,-4:].astype(np.float16)\n",
    "\n",
    "        # All features - stack and scale\n",
    "        all_features = np.hstack((features, cyclical_features))\n",
    "\n",
    "        # Scale feature data to 0-1 scale\n",
    "        scaler = MinMaxScaler()\n",
    "        all_features = scaler.fit_transform(all_features)\n",
    "        # Scalers for temp & pres only\n",
    "        temp_scaler = MinMaxScaler()\n",
    "        temp_scaler.fit(np.array(features[:,0]).reshape(-1,1))\n",
    "        prec_scaler = MinMaxScaler()\n",
    "        prec_scaler.fit(np.array(features[:,1]).reshape(-1,1))\n",
    "        if save:\n",
    "            np.save(repo_path+'/data/processed/spatial_temporal/feature_means', self.data_mean)\n",
    "            np.save(repo_path+'/data/processed/spatial_temporal/feature_stds', self.data_std)\n",
    "            joblib.dump(scaler,repo_path+'/data/processed/spatial_temporal/all_feature_scaler.pkl')\n",
    "            joblib.dump(temp_scaler,repo_path+'/data/processed/spatial_temporal/temp_scaler.pkl')\n",
    "            joblib.dump(prec_scaler,repo_path+'/data/processed/spatial_temporal/prec_scaler.pkl')\n",
    "        \n",
    "        # Recombine & Reshape\n",
    "        self.data = np.hstack((date, lat_lon, all_features))\n",
    "        self.data = self.data.reshape(-1, self.num_features)\n",
    "\n",
    "        \n",
    "    def process_datetime(self, dt_fmt:str='%Y-%m-%d', datetimecol='start_date'):\n",
    "        #print(\"Parsing datetime fields \\n\")\n",
    "        def lookup(s):\n",
    "            \"\"\"\n",
    "            This is an extremely fast approach to datetime parsing.\n",
    "            \"\"\"\n",
    "            dates = {date:pd.to_datetime(date, format = dt_fmt) for date in s.unique()}\n",
    "            return s.map(dates)\n",
    "        self.data[datetimecol] = lookup(self.data[datetimecol])\n",
    "        self.all_dates = np.unique(self.data[\"start_date\"].dt.strftime('%Y-%m-%d'))\n",
    "        \n",
    "    def get_missing_regions(self):\n",
    "        \"\"\" Find regions in lat-lon box which are not modelled geographic regions  \"\"\"\n",
    "        self.all_region_ids = [(lat,lon) for lat in range(self.latmin, self.latmax+1) for lon in range(self.lonmin,self.lonmax+1)]\n",
    "        self.num_total_regions = len(set(self.all_region_ids))\n",
    "        self.missing_regions = list(set(self.all_region_ids) - set(list(self.regions)))\n",
    "        self.missing_regions.sort() \n",
    "        \n",
    "    def mask_missing_regions(self, mask_value=0):\n",
    "        \"\"\" Create masked data for missing region - zero pad (0,1) scaled features \"\"\"\n",
    "        self.get_missing_regions()\n",
    "        masked_rgn_lst = []\n",
    "        for rgn in self.missing_regions:\n",
    "            date_col = self.data.reshape(self.num_regions,-1,self.num_features)[0,:,0].reshape(self.num_timesteps,1) #take same dates\n",
    "            lat_col = np.array([rgn[0]]*self.num_timesteps).reshape(self.num_timesteps,1) #take lat of current region\n",
    "            lon_col = np.array([rgn[1]]*self.num_timesteps).reshape(self.num_timesteps,1) #take lon of current region\n",
    "            feature_cols = np.array([mask_value]*self.num_timesteps*(self.weather_features)).reshape(self.num_timesteps,self.weather_features) #mask weather features\n",
    "            cyclical_cols = self.data.reshape(self.num_regions,-1,self.num_features)[0,:,-self.cyclical_features:].reshape(self.num_timesteps,self.cyclical_features) #take time features\n",
    "            masked_rgn = np.hstack((date_col, lat_col, lon_col, feature_cols, cyclical_cols))\n",
    "            masked_rgn_lst.append(masked_rgn)\n",
    "        masked_rgns = np.concatenate(masked_rgn_lst)\n",
    "        self.masked_rgns_df = pd.DataFrame(masked_rgns)\n",
    "        self.masked_rgns_df.columns = self.col_names\n",
    "        self.masked_rgns_df['region_id'] = list(zip(self.masked_rgns_df['lat'].astype(int), self.masked_rgns_df['lon'].astype(int)))\n",
    "        self.masked_rgns_df['model_region'] = False\n",
    "        \n",
    "    def convert_rgns_data_to_df(self):\n",
    "        \"\"\" Convert to df of shape (num_timesteps*num_regions, num_features). Ordered by region by timestep \"\"\"\n",
    "        self.data = pd.DataFrame(self.data.reshape(-1,self.num_features))\n",
    "        self.data.columns = self.col_names\n",
    "        # Create unique region id\n",
    "        self.data['region_id'] = list(zip(self.data['lat'].astype(int), self.data['lon'].astype(int)))\n",
    "        self.data['model_region'] = True\n",
    "        \n",
    "    def join_masked_regions(self):\n",
    "        \"\"\" Join masked regions df to rgns data df, and sort \"\"\"\n",
    "        self.data = pd.concat([self.data, self.masked_rgns_df])\n",
    "        # Convert date to datetime\n",
    "        self.process_datetime()\n",
    "        # Sort\n",
    "        self.data = self.data.sort_values(by=['lat', 'lon', 'start_date']).reset_index(drop=True)\n",
    "        assert self.num_total_regions == len(self.data['region_id'].unique())\n",
    "        \n",
    "    def get_global_region_tensor(self, save=False):\n",
    "        print(\"Generating global spatial grid \\n\")\n",
    "        spatial_tw_list = []\n",
    "#         self.all_dates = np.sort(self.data['start_date'].unique())\n",
    "        # For every timestep, create binsize*binsize global spatial grid of demand\n",
    "        for counter, time_window in enumerate(self.all_dates):\n",
    "            mask = self.data['start_date'] == np.datetime64(time_window)\n",
    "            pvt_current_time_window = np.flipud(np.array(self.data[mask]).reshape(self.bin_height,self.bin_width,-1))\n",
    "            spatial_tw_list.append(pvt_current_time_window)\n",
    "        # Store global_regions_tensor - stack as tensor: for bin_index [num_timesteps, bin_width , bin_height, number of channels]\n",
    "        self.global_region_tensor = np.stack(spatial_tw_list).reshape(-1,self.bin_height,self.bin_width,pvt_current_time_window.shape[2])\n",
    "        self.global_region_tensor = self.global_region_tensor[:,:,:,:-2]\n",
    "\n",
    "        # Pad outer edge with max spatial granularity\n",
    "        npad = ((0, 0), (self.max_sg, self.max_sg), (self.max_sg, self.max_sg), (0, 0)) # pad (before, after) on region height/width dimensions only\n",
    "        self.global_region_tensor = np.pad(self.global_region_tensor, pad_width = npad, mode='constant', constant_values=0)\n",
    "        # Save to disk\n",
    "        if save:\n",
    "            print(\"Saving global spatial tensor \\n\")\n",
    "            np.save(repo_path+'/data/processed/spatial_temporal/global_region_tensor_scaled_sg'+str(self.max_sg), self.global_region_tensor)\n",
    "    \n",
    "    def preprocess_pipeline(self):\n",
    "        self.standardize_and_scale_data(save=True)\n",
    "        self.mask_missing_regions()\n",
    "        self.convert_rgns_data_to_df()\n",
    "        self.join_masked_regions()\n",
    "        self.get_global_region_tensor(save=True)\n",
    "        del self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5L5rzwDCcse"
   },
   "outputs": [],
   "source": [
    "y = PreprocessTemporalSpatialData(df, locations, col_names, num_regions=514, num_features=15, max_sg=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9ZG9eYeI1W2"
   },
   "outputs": [],
   "source": [
    "y.preprocess_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIVCl-JH7bG8"
   },
   "outputs": [],
   "source": [
    "# np.save(repo_path+'data/processed/full_data_scaled_1979-2018', df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "1_preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
